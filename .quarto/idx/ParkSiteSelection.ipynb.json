{"title":"Applied Reproducible Data Science Processes","markdown":{"yaml":{"title":"Applied Reproducible Data Science Processes","subtitle":"A demonstration using data from Wisconsin lakes","author":[{"name":"Adam Ross Nelson JD PhD","email":"arnelson3@wisc.edu","affiliation":"University of Wisconsin - Madison"}],"date":"January 1, 2025","execute":{"echo":true,"warning":false},"format":{"html":{"toc":true,"toc-depth":3,"toc-title":"Table of Contents","number-sections":true,"code-fold":true,"code-tools":true,"keep-md":true,"embeded-resources":true},"pdf":{"toc":true,"number-sections":true,"fig-width":6,"fig-height":4,"fig-align":"center","documentclass":"article","margin-left":"1in","margin-right":"1in","margin-top":"1in","margin-bottom":"1in","keep-tex":true,"header-includes":"\\usepackage{draftwatermark}\n\\DraftwatermarkOptions{%\n  text={INPROGRESS},\n  color={[rgb]{1,0.6,0.6}}, % light red\n  angle=45,\n  scale=.5\n}\n"},"docx":{"reference-doc":"custom-reference.docx"}},"engine":"jupyter","bibliography":"references.bib","jupyter":"python3"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n<style>\n.draft-watermark {\n    position: fixed; top: 50%; left: 50%;\n    transform: translate(-50%, -50%) rotate(-45deg);\n    font-size: 100px; font-weight: bold; color: #FF0000; opacity: 0.1; }\n</style>\n\n<div class=\"draft-watermark\">INPROGRESS</div>\n\n\nA common question faced by organizational executive leadership (the c-suite), academic researchers, industry professionals, students, and others regarding data science methods or models is what do these methods and models accomplish. Closely related is what does a data scientist do? Or, how does a data scientist provide value to an organization? These are important questions becuase as @raschka2019python point out large companies \"heavily invest in machine learning research applicaionts for *good reason*\" (p. xiii) (emphasis added). This paper presents a novel response to those questions.\n\nI write this paper for at least four audiences who all have a common interest. \n\n- First for organizational leaders who may not seek to employ these methods on their own but who have an interest in developing or acquiring talent who can employ these methods.\n- Second, for academic researchers who are situated in universities, think takes, or research institues and who are often funded by private or government research grants.\n- Third, for industry professionals who may work in for-profit, not-for-profit, or governmental organizations.\n- And fourth for students, at any level of study but who may be newer to the exploration of data science.\n\nThese audiences share a common interest in understanding the practical utility of data science methods and models. Each of thease audiences have a need to understand how data science can offer highly reproducible approaches, results, insights, utility, and other forms of valuable output. To assist in meeting these common objectives I also distribute with this paper its raw markdown, Python code, Jupyter notebooks, and data sources.\n\nThis paper presents an example of work that might have been performed by a data scientist who is working to support policy makers as they oversee the maintenance of public accommodations on lakes located across the State of Wisconsin. This paper's example does not put a model into production because the contexts do not demand a model in production. \n\nThe guiding analytical question and goal for this paper is to use data science as a means to explore which lakes have public services such as a park, beach, or boat landing but perhaps should not. Conversely also to explore which lakes should have public services such as a park, breach, or boat landing but perhaps do not.\n\nThose that have a service but perhaps should not I will describe as overserviced while those that do not have service but perhaps should I will describe as underserviced.\n\n# Literature Review\n\nThis literature review serves to provide an overview of three important contexts that inform this paper's analytical approach including the policy context, the environmental/limnological/geologic context, and the practice of data science.\n\n## Policy Context\n\nThe goal of this project is to inform multiple autorities and decision makers as they work in collaboration to make and implement public policy. This context, where also multiple sources of authority conflict in their interests, may benefit from an analysis that identifies underserviced or overserviced lakes.\n\nThe management of Wisconsin’s lakes is a shared responsibility that involves state, local, and federal authorities, each contributing unique roles and expertise to ensure sustainable and effective oversight. @garn2003why also point to lake management associations and lake districts as a source of authority, management, and oversight.\n\nAt the state level, the Wisconsin Department of Natural Resources (WDNR) serves as a key authority, providing a framework for lake management. The WDNR’s Lake Modeling Suite offers tools for assessing lake health and predicting management strategy outcomes, while the Surface Water Integrated Monitoring System (SWIMS) serves as a comprehensive database for water quality and ecological data [see @wisconsin_dnr_lake_modeling_suite; and @wisconsin_dnr_swims].  These resources are classic examples of those that data science practitioners would reference in executing a study with this study's analytical goals.\n\n::: {.callout-note} \nThe Wisconsin Department of Natural Resources' Surface Water Integrated Monitoring System (SWIMS) exemplifies the kind of data that data scientists often encounter in applied, policy-relevant contexts. Like many administrative and environmental datasets, SWIMS is rich with information but also characteristically messy. Analysts working with these or similar data must address missing values and inconsistencies in field formats, among other oddities. For example, some columns store numbers as strings due to the presence of unit labels (such as \"43 FEET\"), while others contain values outside the expected or valide ranges. Data of this kind demands both exploration and transformation before meaningful analysis can begin.\n:::\n\nThe state does not operate alone [@Thornton2013Stakeholder]. Local city and county authorities play a part in lake management, particularly in implementing localized policies that address specific community needs including whether to establish a park, beach, or boat access. For example, local governments often plan roads that would service the accommodations and oversee zoning law that would regulate which manner of development may be allowed in a given area. \n\nFederal authorities also contribute to the management of Wisconsin’s lakes through regulatory and funding mechanisms. The federal Environmental Protection Agency (EPA) establishes water quality standards and provide financial support for restoration projects under the Clean Water Act [@Elbakidz2021hetero] and similar initiatives. Additionally, federally funded programs contribute to the preservation of critical habitats and mitigation of invasive species, which are pressing concerns for many of Wisconsin’s lakes.\n\nIn addition to state, local, and federal authorities, tribal governments play a role in the management and preservation of Wisconsin’s lakes [@Waller2018Ecology; and @Spear2020Application]. Many of Wisconsin’s lakes are located on or near tribal lands, and the sovereign rights of indigenous nations give them a unique and essential role in overseeing natural resources. Tribal authorities often have deep, place-based knowledge and cultural ties to these waters, which guide stewardship practices [@Chief2014Engaging].\n\nThe overlapping jurisdictions and collaborative efforts among these levels of government underscore the complexity of lake management. For data scientists working in this domain, understanding the policy context is crucial for framing analytical questions, interpreting data, and ensuring that findings are actionable for stakeholders across all levels of governance. Because this analysis may inform policy decisions across various levels of government (state, local, tribal), the output of this analysis may be useful by many.\n\n## Environmental Limnological Geologic\n\nAs a study of Wisconsin lakes, this work intersects at least three major academic and scientific domains: environmental science, limnology, and geology. In practice a data scientist would heavily rely on literature and also consultations with experts who study, know, and understand these domains. Knowledge from literature and from consultation is commonly called \"domain knowledge\" among data science practitioners [@nelson2023confident]. The ability to contextualize data is further emphasized by [@carvalho2024new], who notes that collaboration with domain experts is vital throughout the data science process to ensure that the results are relevant and actionable. \n\nAccording to @bendor2013modeling the following factors figure into park development: \n\n- Road and highway access: The accessibility of a lake by road or highway.\n- Proximity to public utilities: The availability of public utilities including electricity, septic, sewer, and similar.\n- Size and scope of the park: Larger parks with more complex infrastructure will take longer to plan and build.\n- Land acquisition: Acquiring land can be time-consuming, especially if it involves negotiations, legal proceedings, or eminent domain.\n- Environmental impact assessment: Environmental reviews and permits can add significant time to the process.\n- Funding: Securing funding for land acquisition, design, and construction can be a major hurdle and can delay the project.\n- Public input and approval: Public hearings and comment periods can extend the timeline, especially if there are significant concerns or opposition.\n- Construction timeline: The actual construction phase can vary depending on the complexity of the project, weather conditions, and other factors.\n\nEach of these domains contributes essential perspectives to understanding the complex systems governing lake ecosystems and their management.\n\n::: {.callout-note}\nDomain knowledge refers to the specialized understanding and expertise within a particular field or subject area. In the context of this study, domain knowledge spans environmental science, limnology, and geology—fields that provide the essential insights needed to analyze and interpret data about Wisconsin lakes. It allows data scientists to frame relevant questions, choose appropriate methodologies, and ensure that their findings are both accurate and actionable for decision-makers. [@nelson2023confident].\n:::\n\nThe environmental domain encompasses the broader ecological interactions affecting Wisconsin lakes. This ecology includes factors such as land use, agricultural, water quality, and climate changes. Limnology, the study of inland waters, focuses specifically on the biological, chemical, and physical characteristics of lakes and their surrounding environments. The geologic domain provides insights into the underlying structure and formation of Wisconsin’s lake basins.\n\nThe interplay of environmental, limnological, geologic (and other) factors defines the unique character of each lake. Effective policy and management decisions require an integrated understanding of these domains, supported by data-driven insights. By combining these perspectives, this study aims to model how a data science practitioner's work necessarily involves reference to knowledge not specific to data science in collaboration with others.\n\n\n## The Data Science Process\n\n> The goal of generating new knowledge is at the heart of all scientific disciplines, including data science. It is about finding patterns and relationships in the data that can help us make better decisions or solve complex problems. [@nelson2023confident, p 67]\n\nFor a range of reasons data sciecne is also more \"experimental\" in its approaches [@Martins2021earlyprediction] than more traditional inferential statistical work. For example, according to @Martins2021earlyprediction in regards to model or algorithm selection the best machine learning technique is dependent \"on the dataset and on the formulation of the problem\" (p. 172). According to @Breiman2001culture who described more traditional approaches as \"data modeling\" and the more experimental approahces as \"algorithmic\" modeling a key distinction between these two approaches is that for data modeling the main metric as to what may constitute a \"good\" model is a look at goodness of fit while the key metric for algorithmic modeling is predictive accuracy.\n\n@Breiman2001culture defends so-called algorithmic modeling culture by suggesting that the focus among those in the data modeling culture have brewed an environment which has \"lead to irrelevant theory,\" \"questionable scientific conclusions,\" limited or stymied the use of \"more sutiable agorithmic models,\" and prevented work \"on exciting new problems\" (p. 299-200).\n\nA focus on accuracy is not a trivial distinction. The distiction underscores \nthe practical utility of what @Breiman2001culture refers to as algorithmic modeling in addressing complex problems. This paper’s research question, treated as a thought experiment, highlights the utility of prioritizing accuracy over measures like goodness of fit. Specifically, our goal is to produce two actionable lists of lakes:\n\n1. **Overserviced Lakes** Lakes that currently have public accommodations but perhaps should not. These will be lakes that our model incorrectly predicts as lacking public accommodations.\n2. **Underserviced Lakes** Lakes that lack public accommodations but perhaps should have them. These will be lakes that our model incorrectly predicts as having public accommodations.\n\nThus, given this paper's analytical question the results of interest will be incorrect predictions. The flexibility of data science and it's algorithmic modeling culture permits a tailored approach finely tuned based on the needs of policymakers. \n\nFor instance, if policymakers prefer longer lists of lakes to consider, we can adjust the model through parameter tuning and feature selection to increase error rates, thereby expanding the lists. Conversely, if shorter, more focused lists are desired, we can optimize the model to reduce error rates. \n\nAdditionally, as this paper does, we can refine our approach by analyzing the probability of class membership rather than relying solely on binary predictions. A close look at probabilities permits further customization of list lengths to better meet policy objectives.\n\nIn order to solve the complex problem of deciding which lakes need more services and which lakes may have services but not need them, this paper explores patterns and relationships in Wisconsin Department of Natural Resources (WDNR) data by following the eight step data science process as documented by @nelson2023confident and reproduced here in @fig-data-sicence-process. The paper's new knowledge will be a list of lakes that are potentially overserviced along with another list of lakes that may be underserviced.\n\nThis process, as illustrated in @fig-data-sicence-process starts at the upper left under the heading \"Question or Problem\" and moves clockwise as an interative cycle as it also further consists of the following:\n\n1. **State an analytical question or a business problem to solve.**\n2. **Look + check around.** Explore if the question has been answered or if the problem has been solved. Also look to see what other similar questions or problems have been studied. Identify which methods have, or have not, been successful in the past. Also identify potential data sources.\n3. **Justify the work.** Examine the scope of the question or the problem. Determine how answering the question or solving the problem provides value. Determine what additional revenues or efficiencies an answer to the question or a solution to the problem may bring. If the question has previously been answered or the problem previously solved decide if now is a good time to replicate the work (Is there now new data? Or, are there now new methods available?)\n4. **Wrangle the data.** This involves finding, collecting, extracting, summarizing, validating, formulating, organizing, reshaping, coding, and recoding the data.\n5. **Select and apply a method.** Determine which methods will best answer the analytical question or solve the business problem and then apply those methods.\n6. **Check and recheck.** It is important to look back at this stage. Examine if the analytical question or business problem have been properly stated. Determine if there is new information that may have been missed, discounted, or overlooked in earlier stages. This stage may often involve seeking external input on in-progress work.\n7. **Interpret the results.** Here the process involves answering the original analytical question or proffering a solution to the specified business problem.\n8. **Dissemination.** The last stage involves either, or both disseminating the results or putting them into production.\n\n\n## Dissemination, Not Production\n\nThis paper's example does not exemplify putting a model into production. Below is further explanation as to why putting a model into production is not appropriate for this paper, and indeed the same is also true many papers using data science's algorithmic mode of science.\n\nThis paper's environmental/limnological/geologic contexts are a primary reason why the work here does not involve putting a model into production. A model in production is useful when a business systems, often driven by software systems, require a prediction that can serve as a tool in making a decision. Or in contexts when information is frequently or rapidly changing and the use case calls for using a prediction as a recommendation or an automated decision.\n\n::: {.callout-note}\nA model in production is most useful in environments where predictions must be made repeatedly in response to rapidly changing data. These contexts often involve automated systems or decision-making pipelines—such as in weather forecasting, energy grid optimization, or real-time environmental monitoring—where data streams update frequently, and predictions must adapt continuously. For example, a model deployed in production for natural resources management might monitor streamflow or rainfall to inform automated alerts for flood risks or fire danger. In such cases, real-time data ingestion and dynamic model retraining are essential to the utility of the prediction.\n:::\n\nA description of Wisconsin's lakes from over fourty years ago remains as true today as it was when first published. @lillie1983limnological describe the typical Wisconsin lake as \"natural in origin, equally likely to be of seepage or drainage and stratified or mixed in basic lake type and probably located in the northern half ot the state\" (p. N).\n\nIn the case of Wisconsin's lakes it is not often that a new lake will appear on the map. According to data utilized in this paper's analysis from the WDNR there are {{< var number_of_lakes >}} lakes located throughout the state of Wisconsin. The number of lakes has for decades frequently been reported at a rounded 15,000 (@lillie1983limnological). The number of lakes in Wisconsin, or any geographic region, does not change often. Thus, in this case it is sufficient to train and test a model once on the existing data (which we do not expect to change often).\n\n# Data + Method\n\nAs discussed above the method for this project involves a one-time analysis. Specifically the output will be a list of lakes that do have public accommodations but are more similar to lakes that do not have public accommodations (underserviced). A companion output will be a list of lakes that do not have public accommodations but that are more similar to lakes that do (overserviced).\n\nHere I first discuss the data and then also the method that will produce a list of lakes that should be further considered for the addition of one or more public accommodation and a list of lakes that might benefit from public accommodation closure or retirement.\n\n## WDNR Data {#sec-wdnr-data}\n\nThe WDNR publishes the data for this analysis [@wisconsin_dnr_lake_pages]. The WDNR supports this data, uses it for a full range of purposes [@wisconsin_dnr_swims], and it receives contributions from \"citizen science\" initiatives [@kretschmann2011citizen].\n\nA characteristic lake in this data is {{< var char_lake_name >}} which is about {{< var char_lake_size >}} in acres, up to {{< var char_lake_max_depth >}} deep, (with a mean depth of {{< var char_lake_mean_depth >}}), and located at {{< var char_lake_latitude >}} latitude by {{< var char_lake_longitude >}} longitude in {{< var char_lake_county >}} counties.\n\nBelow @fig-wisconsin-lakes-mapped shows the geographic distribution of lakes across Wisconsin. The lakes with a public beach, park, or boat landing show as dark `x` marks while the lakes with no public beach, park, or boat landing show as small `o` marks. Consistent with estimations from @lillie1983limnological we see in @fig-wisconsin-lakes-mapped that 50% of Wiscosin's lakes are above north 45.5 degress latitude. \n\n::: {.callout-note}\nTo create a geographic visualization of Wisconsin’s lakes, this analysis overlays lake location data onto a geospatial outline of the state. The process relies on several essential tools and geospatial data formats, including GeoJSON, GeoPandas, and the use of a coordinate reference system (CRS) to ensure spatial consistency.\n\nThe geographic outline of Wisconsin is sourced from a publicly available GeoJSON file hosted on GitHub. GeoJSON is a widely used format for encoding geographic data structures using JSON (JavaScript Object Notation). It stores spatial features such as points, lines, and polygons along with associated attribute data. In this case, the polygon defining the boundary of the state of Wisconsin is used as a background layer in the final map.\n\nI provide more thorough discussion of the processes and techniques for building this map visual in the appendicies.\n:::\n\n## Preparation For Analysis\n\nThe following procedure prepares this data for analysis. Starting with a fresh copy of the data from WDNR the code first reports a list of the existing column names and then creates a new set of column names that are more efficient to reference in code. When reviewing the first five columns it appears that max depth and mean depth are stored as strings and contain extraneous text (`\"FEET\"`) data. The code proceeds to remove that extraneous text and then converts the data type to float.\n\nFor a more efficient analysis the code then converts the `haslanding`, `hasbeach`, and `haspark` variables to binary where a `1` replaces the WDNR provided `Yes` value and a `0` replaces the WDNR provided `No` value. The code also creates a `hasservice` variable that is a composite which reports `1` if any of `haslanding`, `hasbeach`, or `haspark` are true and `0` otherwise. The `hasservice` variable will be the primary outcome variable in this analysis.\n\nThe code also inspects the `lat` and `long` data to discover that some values are out of the expected possible range. Wisconsin's southwest corner is at approxaibately 42.5°N by 92.89°W while its northeast corner is at approxaimtely 47.1°N by 86.25°W. These lake data from WDNR include values in the `lat` and `long` columns beyond those ranges. As a preliminary step, where these WDNR data report positive longitude the anlaysis assumes that the intended value was negative to correspond with Wisconsin's location in the western hemisphere. Upon converting the postive longitudinal data values to negative, the remaining non-zero values fall within the expected geographic range. \n\nA total of {{< var zero_coords_count >}} records reported 0°, 0° coordinates. The code also replaces these out of range 0°, 0° data with longitude and latitude values associated with each lake's county coordinates from gigasheet.com. A small number of records contained no county data (or data from multiple counties) and thus the code drops these remaining records from the analysis.\n\nThe newly renamed `fish` column contains a comma separated list of fish species found in each lake. This column also contains a gramatically correct \"and.\" To convert these fish data to an array of dummy columns the code first replace the \"and\" with a comma via `pd.str.replace(' and',',', ')` and then uses `pd.str.get_dummies(sep=', ')`.\n\nThis code also manages missing data as described above in @sec-wdnr-data which describes the data as it was in its oroginal form from WDNR. Three extrememly large lakes in the .018th percential including Lake Winebago are removed. A final inspection of summary statistics is provided in @tbl-prepared-summary.\n\n## Exploratory Data Analysis\n\nExploratory Data Analysis (EDA) serves as a critical bridge between raw data and later more formal analysis and data modeling. This EDA is the stage at which a scientist engages directly with data to uncover initial patterns, surface anomalies, identify missing values, and begin assessing the structure and relationships among variables. @gutman2021becoming describe exploratory data analysis as \"an ongoing process\" (p. 52). Thus EDA is not merely a preliminary step, but rather a dynamic and iterative component of the broader data science workflow. This ongoing process allows the analyst to refine questions, revisit assumptions, and incrementally develop insight into the nature and quality of the data.\n\nAs @nelson2023confident notes, \"without at least some preparation, an exploratory analysis might reveal less than fully useful insights. Simultaneously, without at least some exploration it is not fully possible to know what preparation will be necessary before a full analysis\" (p. 85). This is a *which comes first* problem; a proverbial *chicken or egg* question. For example, in this paper a handful of data manipulations have already been described and executed above, all of which required at least some exploration. Proper execution of EDA early and often through the course of a project guides both data preparation and the analysis along the way. Below is a more formal and analytical exploration of the data aimed at understanding which features may be useful in a predictive algorithm.\n\n**TODO: Add remaining columns to this table. See page 20ish.**\n\n@tbl-prepared-summary presents summary statistics for these lakes data from the WDNR, segmented by whether or not the lake has public services (boat landing, beach, or park). The table reports the mean and standard deviation for each variable within both groups in the left and middle columns, along with pooled overall statistics on the two far right columns. This table provides a summary that can assist in evaluating which variables may serve as useful predictors in classifying or predicting whether a lake has public services.\n\nFor example, lakes with public services are, on average, much larger than lakes without services (126.1 vs. 15.6 acres), and the pooled mean is 51.4 acres. The standard deviation is also substantially higher among lakes with services, reflecting greater variability in size. This  difference suggests that lake size may be a strong candidate as a predictive variable, with larger lakes being much more likely to have public accommodations. Similarly maximum depth (maxdepth), mean depth (meandepth), along with the presence of some fish speacies may also offer predictive value.\n\n## Visual Exploratory Data Analysis\n\nTo explore how each variable may predict the presence of a public service on any of Wisconsin's lakes I produce series of categorical violine plots in @fig-violin-plot. This figure further illustrats how the presence of public services such as boat landings, beaches, or parks may relate to five continuous features of each lake: size, maximum depth, mean depth, latitude, and longitude. Each plot shows the distribution of these features' natural log values for lakes with and without public services.\n\nSpecifically lake size, maximum depth, and mean depth, the top three plots in @fig-violin-plot show differences between lakes that do and do not have public services. In each of these cases, lakes with services (in orange) tend to be shifted to the right along the x-axis, meaning they are generally larger and deeper than lakes without services (in blue). Not only are the central tendencies higher for lakes with services, but the spread of values is also wider, particularly for size, suggesting a greater diversity in lake sizes among serviced lakes. These differences imply that lake size and depth may be strong predictors for whether a lake has public services.\n\nOn latitude and longitude, the bottom two plots of @fig-violin-plot, show less pronounced differences. There is modest separation in the distributions for lakes with and without services, particularly in latitude, where lakes with public services appear to be slightly more concentrated in certain geographic bands. This may reflect regional planning priorities or population density factors but likely provides less predictive value than physical lake characteristics.\n\n::: {.callout-note}\nThe continuous variables displayed in Figure 4 have long-tailed, highly-skewed distributions. To make the visualizations more interpretable and to reduce the influence of extreme values, I applied the natural logarithm transformation. This common transformation compresses the scale of larger values. The result is a more readable distribution.\n:::\n\nThese visual explorations serve as an important visual check on the potential predictive power of each feature when modeling the presence of public service. The greater the separation between the two distributions, the more likely that variable will be useful in a classification task. Based on the WDNR data, features like size, max depth, and mean depth appear to have strong potential as predictors, while latitude and longitude may contribute some additional nuance when combined with other variables.\n\nOne additional customary plot used in exploratory data anlysis is the pair plot. Below @fig-logpairplot_1 again reveals the same patterns noted above in @tbl-prepared-summary and @fig-violin-plot.\n\n## Binary Variable Exploration\n\nThese WDNR data also consist of multiple columns of a binary nature, reporting the presence of specific fish species including catfish, largemouth bass, musky, northern pike, panfish, smallmouth bass, sturgeon, trout, and walleye. Instead of exploring the potential predictive value of these binary with violin plots we must turn to other options such as chi square analysis which is well suited to test the whether the presence of a publice service may be a function of the presence of any given species.\n\n## Machine Learning Predictions As Recommendations\n\n@james2023introduction explain that \"broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs\" (p. 1). Data science methods genreally aim to leverage a series of features which can reliably predict an outcome. \n\nIn this paper's analysis the outcome is whether a lake has as public service while the predictors are each lake's size in acers, maximum depth, mean depth, latitude, longitude, type ({{< var lake_type_vals >}}), clarity ({{< var lake_clarity_vals >}}), and the presence of specific fish species ({{< var fish_species_list >}}).\n\nThrough the use of a predictive algorithm that measures similarity this paper's analysis will first create a model that looks to predict which of Wisconsin's lakes have a public service and which of Wisconsin's lakes do not have a public service. Inevitably there will be some errors.\n\nOne set of errors will be lakes that do not have a service but that the model predicted would (becauase they are similar to others that do) which we will call overserviced. Likewise another set of errors will be lakes that do have a service but that model predicted would not (becuase these other lakes are more similar to those that do not have service) which we will call underserviced.\n\n## k-Nearest Neighbors (KNN)\n\nTo classify whether a lake should or should not have a public service, this paper turns to a classic algorithm known as k-Nearest Neighbors (KNN). More traditionally, this algorithm serves to predict class membership based on an observation's similarity to other observations of a known classification.\n\nKNN is a non-parametric, instance-based learning algorithm that does not make assumptions about the underlying distribution of the data. Instead, it classifies instances based on the majority class of their closest neighbors in a feature space. Which is a technical manner of explaining that KNN is a way for a computer algorithm to make predictions or conclusions about an object based on what it \"sees\" (or measures using euclidian distances) as other similar objects. \n\nA simplified version of the KNN process, as it operates for this paper, is to first observe an unknown lake and its characteristic features. Suppose the unknown lake is less than one acre in surface area and also less than 6 feet deep at its max. Then further suppose that 99% of all lakes less than an acre in size and also around 6 feet deep (+/- one foot) have no public service. Thus it would appear that the unknown lake is similar to lakes with no service and accordingly the algorithm would predict the uknown lake as one that also would have no public services.\n\nIn data science we use the term *non-parametric* to describe predictive algorithms, such as KNN, which have no fixed formula. Being absent a fixed formula distinguishes the non-parametic approach from parametric approaches, such as logistic regression, for example. @james2023introduction describe KNN as one of \"the simplest and best-known non-parametric methods\" (p. 111).\n\nModels that are *instance-based* make decisions by comparing one instance (or given this paper's data any instance of a given lake) to others instances (other lakes). During the model fit procedure the algorithm memorizes the training data by storing as a reference all training instances. A *feature space* is a term for the way we describe the multi-dimensional, or multi-variate, nature of the predictive features and their values.\n\n::: {.callout-note}\nThis distance metric must not be conflated for geographic distance. For example, two lakes located on opposite sides of the state may still be considered \"close\" in feature space if they share similar size, depth, water clarity, and fish populations. This abstract notion of distance allows KNN to make predictions based on overall similarity in characteristics, rather than geographic location alone.\n:::\n\nIn this paper's data, the feature space includes dimensions such as lake size, maximum depth, mean depth, latitude, longitude, water clarity, and the presence of specific fish species. The KNN algorithm uses this feature space to calculate distances between lakes and identify their nearest neighbors, which helps classify them based on their similarity.\n\n# Analysis\n\nThis analysis uses the open-source Python package Scikit-learn (https://scikit-learn.org), a widely adopted and well-documented framework for implementing machine learning models, including k-nearest neighbors (KNN). Scikit-learn supports reproducible and transparent research, making it ideal for applications in data science and policy evaluation.\n\n**TODO: Add Sklearn documenation as a source.**\n\nThe analytical procedures in this paper align with those outlined in Chapter 11 of @nelson2023confident and Chapter 3 of @raschka2019python. Most machine learning workflows begin by splitting the available data into training and testing sets. The training set allows the model to learn patterns and relationships in the data, while the testing set remains untouched until final evaluation. This split helps ensure that performance estimates reflect how the model will generalize to new data not seen during training.\n\nThe training and testing sets also support model parameter selection. In this case the parameter to optimize is the optimal number of *k* neighbors in the KNN algorithm. As is also customary, to avoid information leakage, this analysis performs data preprocessing after the data has been split. Binary features (such as fish species presence) require no further transformation. However, nominal features such as clarity and type are converted into dummy variables using one-hot encoding, and all continuous predictors are rescaled using a standardization procedure to ensure they contribute equally to distance calculations.\n\nFollowing transformation, the next step involves conducting a parameter search to identify the most effective value for *k*, the number of nearest neighbors used in the KNN algorithm. In most cases, the customary approach is to select the smallest value of *k* that also minimizes prediction error. A smaller *k* yields a simpler, more interpretable model. However, this paper intentionally selects a less-well performing k value to relize a more complex model. The reasoning behind this choice is practical: a more complex model yields a finer-grained and more complete distribution of predicted probabilities. These probabilities in turn, support a more nuanced analysis of false predictions. As the anslysis seeks to identify lakes that do not have public services but appear similar to those that do, and vice versa. This added granularity enhances the utility of the model in producing actionable policy recommendations.\n\nAs a final step, this analysis uses a two-fold cross-validation with symmetric evaluation, which ensures that every lake is evaluated as an out-of-sample observation exactly once. In the first fold, the model is trained on half of the data and used to predict outcomes for the other half; in the second fold, the roles are reversed. This method yields a complete set of out-of-sample predictions, which allows us to identify false positives (lakes without public services that resemble lakes that do) and false negatives (lakes with public services that resemble those that don’t). These classification errors serve as the empirical foundation for the policy recommendations presented later in this paper.\n\n## Train Test Split Procedure\n\nThe SciKit Learn user guide states plainly that \"Learning the parameters of a prediction function and testing it on the same data is a methodological mistake\" @scikit-learn-cross-validation2023. This mistake would usually lead to a model that has *overfit*. Instead of finding the general functional relationships between predictor features and an outcome, a model that has overfit to the data, has come close to memorizing the training data.\n\nBy first fitting a model on a subset of training data and then using a separate hold out subset as a test, the procedure results in a more objective opportunity to fairly evaluate the predictive abilities of a model. The procedure ensures that a model which performs well on training data also later performs well on new data yet to be generated in future production settings.\n\n## Rescale Continuous Data\n\nAccording to @raschka2019python \"the majority of machine learning... algorithms behave much better if features are on the same scale\" (p. 124). In the case of KNN, an algorithm that relies on calculating distances between data points in the feature space, if the features spread across vastly different scales, those with larger ranges will dominate the distance calculations. \n\nFitting a KNN on features that are differently scaled may potentially lead to biased classifications. For instance, if one feature represents lake size in acres (ranging from tens to thousands) and another represents water clarity on a scale of 1 to 100, the size feature will disproportionately influence the nearest neighbor determination. Thus, For KNN, rescaling ensures that all features contribute equally to the distance metric [@nelson2023confident, p. 317].\n\nTo address this issue, continuous data is typically rescaled to 0 to 1, -1 to 1, or to z-score values which places the mean at 0 and then shifts values so there is a standard deviation of 1. In this paper I proceed with a 0 to 1 scale.\n\n## Fit Base Model\n\nBefore proceeding with parameter tuning or model refinement, it is often useful to fit a base model using an arbitrary but reasonable choice of parameters. In the case of k-nearest neighbors (KNN), selecting a base *k* such as *k* = 19. This base model provides an initial benchmark for model performance. This step, though technically optional, serves several important purposes within the broader analytical process.\n\nFirst, fitting a base model allows the analyst to ensure that the pipeline—from data preprocessing to model training and evaluation functioned as intended. Errors related to data structure, scaling, encoding, or other preprocessing steps often surface during this preliminary fit. The base model thus acts as a diagnostic opportunity to detect problems before introducing additional complexity through cross-validation, parameter searches, or other hyperparameter tuning efforts.\n\nSecond, the base model offers a reference point for evaluating the value added by subsequent modeling decisions. For example, if the accuracy or error rate of a tuned model differs only marginally from that of the base model, the analyst may reconsider the complexity or interpretability trade-offs involved in optimization. Conversely, large improvements over the base model suggest meaningful gains that justify further refinement.\n\nFinally, the base model supports replication and transparency by providing a fixed and reproducible result that others can use to validate or extend the work. By producing and recording model performance with arbitrary but documented parameters, the analysis builds a foundation upon which subsequent results can be compared, especially in applied contexts where interpretability and policy implications matter as much as technical performance.\n\nIn short, while fitting a base model with arbitrary *k* is not strictly necessary for most analyses, it offers extensive practical value in building a rigorous, transparent, and well-structured analysis.\n\nThis base model yeild {{< var base_accuracy >}} accuracy (correct classifications). There were {{< var base_false_pos >}} false positive predictions and {{< var base_false_neg >}} false negative predictions. These base metrics can serve as a helpful reference when evaluating futher results below.\n\n## Evaluate + Search for Optimal K\n\nThis portion of the analysis implements a parameter search to determine how the choice of *k* (the number of neighbors considered in the KNN classification algorithm) affects model performance. The goal is to identify a value of *k* that yields relatively low classification error, thereby improving the model’s predictive accuracy. As discussed above, for this paper I will not choose the lowest error rate in order to support a fully nuanced analysis.\n\nThis portion of the analysis begins by initializing an empty list named `error_rates` to store the error rate associated with each value of *k*. The `for` loop then iterates through odd-numbered values of *k* from 1 to 99. Odd values avoid tie votes in binary classification. For each iteration, the code instantiates a new `KNeighborsClassifier` model using the current value of *k* and fits it to the training data.\n\nOnce trained the code predicts classifications for the testing set (`X_test`), then calculates the error rate as the proportion of incorrect predictions, and appends that result to the `error_rates` list. By the end of the loop, the list holds the model's error rates across a range of *k* values.\n\nA subsequent block of code generates @fig-error_rates, a visual representation of the results using Matplotlib. @fig-error_rates uses a dotted blue line with ‘x’ markers to show how error rates change as *k* increases.\n\nBy examining @fig-error_rates's curve, an analyst can make an informed decision about which *k* values to consider for the final model—balancing error rate, model simplicity, and practical interpretability. Given these results I choose a *k* value of 29 consistent with the procedure outlined above.\n\n## Two-fold cross-validation; Symmetric evaluation\n\nIn executing this procedure I first split the data into two equal halves. In the first fold, I train the model on the first half and predict on the second; in the second fold, the I reverse the process.\n\nAs before, within each fold, continuous variables are standardized using `StandardScaler` and nominal categorical variables are transformed via one-hot encoding using `OneHotEncoder`.\n\nAfter each fit, train, and predict I record both the prediction and also the probability of membership in the `hasservice` class. Finally, I combine predictions from both folds and merged them back into the original data. The result is a full set of out-of-sample predictions and prediction probabilities for each observation.\n\n# Results\n\nThis analysis produced {{< var false_preds_count >}} false predictions. A full list of all the lakes analyzed in this study and the predictive results accompanies this project in a file called `output_data.csv`. \n\n## Underserved Lakes\n\nUnderserved lakes are the lakes that this analysis falsely predicted not to have services, but that actually do have services. Because the results produced {{< var false_positive_count >}} false positives I further refine the definition of underserved lakes to include those that had a probability of service greater than .80. @tbl-overserviced-lakes provides a full list of underserviced lakes.\n\n\n<div style=\"font-size: 12px;\">\n\n</div>\n\n@tbl-underserviced-lakes presents a list of lakes identified by KNN model as false positives, or as lakes that were predicted to have public services but do not currently offer them. Each row includes the lake's name, its surface area in acres (`size`), geographic coordinates (`lat` and `lon`), the county or counties in which the lake is located, and the model's estimated `probability`. \n\nThis `probability` value is a key element in the table: it represents the proportion of the 29 nearest neighbors (as set by the optimal *k* in the model) that do have a public service. For example, a probability of `0.827586` implies that 25 out of 29 neighboring lakes in the feature space had a public service, making the model highly confident that the lake in question should as well.\n\nLakes such as Kilbourn Flowage, Kettle Moraine Lake, and Fence Lake have especially high probabilities—approaching or reaching 1.0—indicating that all of their most similar lakes in the multi-dimensional feature space do have services. These high-probability false positives are particularly important because they suggest a strong pattern of similarity when compared to serviced lakes and thus may warrant prioritization for future investments in public amenities.\n\n## Overserviced Lakes\n\nThe opposite of underserved lakes, overserviced lakes are those that the model suggested would not have services but do (the false negatives). With {{< var false_negative_count >}} false negatives it is also convenient to refine this definition. To reduce the number of lakes for consideration as overserviced I further refine the definition of overserviced lakes to only include those that had a probability of service less than .03. @tbl-underserviced-lakes provides a full list of these overserviced lakes.\n\n\n<div style=\"font-size: 12px;\">\n\n</div>\n\n@tbl-overserviced-lakes identifies lakes classified as false negatives by the KNN model. These lakes do have public services, but the model predicted they would not. In this analysis, such lakes are referred to as overserviced, meaning they are dissimilar—based on the features available—to most other lakes that also have services.\n\nAs was also with @tbl-underserviced-lakes this `probability` column reflects the proportion of the 29 most similar lakes (i.e., nearest neighbors in the multi-dimensional feature space) that had public services. A `probability` of 0.0 implies that none of the 29 neighbors had a service.\n\nFor example, Fountain City Bay and Ceylon Lagoon offer public services but are quite like at least 29 other unserviced lakes. Ultimately, @tbl-overserviced-lakes and @tbl-underserviced-lakes provide a data-driven starting point for reevaluating the current distribution of public services across Wisconsin’s lakes by identifying outliers that may represent opportunities for improved alignment with service needs. Whiel @fig-knn-false-predictions shows the location of these underserviced and overserviced lakes.\n\nThe map in @fig-knn-false-predictions presents a visualization of false predictions generated by the KNN classification model trained to identify which Wisconsin lakes have public services—such as boat landings, beaches, or parks—based. Specifically, it highlights two important categories of interest for policy consideration: underserved lakes and overserviced lakes.\n\n# Further Discussion\n\nLakes marked with blue \"x\" symbols represent false positives, or underserved lakes. These underserved lakes are those that the model predicted to have public services, because they are similar to other lakes that do, but that in reality do not have public services. These lakes may warrant further investigation as candidates for the addition of public accommodations.\n\nConversely, lakes marked with black dots represent false negatives, or overserviced lakes. These overserviced lakes are those where the model predicted no public services should be present but that are currently are serviced with boat landings, beaches, or parks. These may be examples of lakes whose public services might be considered for decommission or retirement.\n\nBoth underserved and overserved lakes are widely distributed throughout the state. This spatial analysis offers a critical bridge between predictive modeling and those audiences who may consume this information as they look to make and implement lake management policy. By identifying specific lakes for which the model's prediction diverges from current reality, this map serves as a guide for natural resource managers, planners, and local officials for further review.\n\n## Limitations + Weaknesses\n\n{{< lipsum 3 >}}\n\n## Suggestions For Further Work\n\n{{< lipsum 2 >}}\n\n\n\n\n# Summary + Conclusions\n\n{{< lipsum 4 >}}\n\n# Appendicies\n\n**Mapping Wisconsin’s Shape and Lake Locations**\n\nTo generate the maps featured in this analysis, the shape of the state of Wisconsin was retrieved from a public **GeoJSON** file hosted on GitHub. GeoJSON is a geospatial data format based on JSON (JavaScript Object Notation) that stores geographic features such as points, lines, and polygons. In this case, the boundary polygon of Wisconsin serves as the base layer for the map.\n\nThe file is read into a **GeoDataFrame** using the `GeoPandas` library (`gpd`). GeoPandas is an extension of Pandas that enables spatial operations and plotting by introducing a `geometry` column capable of storing shapely geometries. A simple function was written to check whether the Wisconsin GeoJSON file already exists locally and, if not, to download and store it from a known remote source.\n\nLake location data from the Wisconsin Department of Natural Resources (WDNR) was merged with additional columns indicating the presence of public services. Using `gpd.points_from_xy()`, each lake’s longitude and latitude values were converted to a point geometry, and the data were also cast into a GeoDataFrame.\n\nTo ensure proper alignment on the map, both the Wisconsin boundary and the lake point geometries were assigned the same **Coordinate Reference System (CRS)**: EPSG:4326, which corresponds to the standard WGS84 system using degrees of latitude and longitude.\n\n- EPSG:4326 refers to a specific coordinate reference system (CRS) that is widely used for geographic data. It is part of a registry maintained by the European Petroleum Survey Group (EPSG), which assigns numeric codes to standard spatial reference systems. EPSG:4326 specifies that coordinates are expressed in degrees of latitude and longitude, based on the WGS84 datum.\n\n- WGS84, or the World Geodetic System 1984, is a global reference system used by GPS and many mapping applications. It defines the shape of the Earth as an ellipsoid and provides a consistent framework for locating points on the Earth's surface. When spatial data is aligned to WGS84, it can be accurately mapped and compared across different datasets and tools, making it a common default in global mapping applications and open geospatial data formats.\n\nPlotting was accomplished using **Matplotlib** and **Seaborn**, with lake points styled according to the presence or absence of public services (e.g., boat landings, beaches, or parks). This overlay provides a spatial perspective that supports both exploratory analysis and the interpretation of model predictions.\n\n# Scratch Space\n\n# Outtakes\n\nThis paper demonstrates a data science process that doesn't require a continually updated model. Unlike situations where data changes frequently (e.g., online shopping recommendations), the number of lakes in Wisconsin is relatively static. Our goal is to provide a one-time analysis identifying potential discrepancies between existing public services and lake needs. This information will be valuable for policy discussions across various government levels, where a single, well-documented analysis is more suitable than a dynamic model.\n\n\n\n\nMoreover, according to @Srinivas2022feature domain knowledge plays a critical role in feature engineering, a key step in the data science pipeline. Srinivas et al. argue that while automation can assist in expanding the feature space, it cannot replace the human ability to apply domain-specific knowledge to identify meaningful features. This sentiment is echoed in the work of @Waller2013bigdata who assert that the intersection of data science with specific domains necessitates a deep understanding of both the data and contexts in which it is applied. Without this understanding, data scientists may struggle to derive insights that are both relevant and useful.\n\n\n\"Overfitting is a common problem in machine learning, where a model performs well on training data but does not genrealize well to unseen datas (test data). If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters, leading to a model that is too complex given the underlying data.\" [@raschka2019python, p 75]\n\n# References\n","srcMarkdownNoYaml":"\n\n<style>\n.draft-watermark {\n    position: fixed; top: 50%; left: 50%;\n    transform: translate(-50%, -50%) rotate(-45deg);\n    font-size: 100px; font-weight: bold; color: #FF0000; opacity: 0.1; }\n</style>\n\n<div class=\"draft-watermark\">INPROGRESS</div>\n\n# Introduction\n\nA common question faced by organizational executive leadership (the c-suite), academic researchers, industry professionals, students, and others regarding data science methods or models is what do these methods and models accomplish. Closely related is what does a data scientist do? Or, how does a data scientist provide value to an organization? These are important questions becuase as @raschka2019python point out large companies \"heavily invest in machine learning research applicaionts for *good reason*\" (p. xiii) (emphasis added). This paper presents a novel response to those questions.\n\nI write this paper for at least four audiences who all have a common interest. \n\n- First for organizational leaders who may not seek to employ these methods on their own but who have an interest in developing or acquiring talent who can employ these methods.\n- Second, for academic researchers who are situated in universities, think takes, or research institues and who are often funded by private or government research grants.\n- Third, for industry professionals who may work in for-profit, not-for-profit, or governmental organizations.\n- And fourth for students, at any level of study but who may be newer to the exploration of data science.\n\nThese audiences share a common interest in understanding the practical utility of data science methods and models. Each of thease audiences have a need to understand how data science can offer highly reproducible approaches, results, insights, utility, and other forms of valuable output. To assist in meeting these common objectives I also distribute with this paper its raw markdown, Python code, Jupyter notebooks, and data sources.\n\nThis paper presents an example of work that might have been performed by a data scientist who is working to support policy makers as they oversee the maintenance of public accommodations on lakes located across the State of Wisconsin. This paper's example does not put a model into production because the contexts do not demand a model in production. \n\nThe guiding analytical question and goal for this paper is to use data science as a means to explore which lakes have public services such as a park, beach, or boat landing but perhaps should not. Conversely also to explore which lakes should have public services such as a park, breach, or boat landing but perhaps do not.\n\nThose that have a service but perhaps should not I will describe as overserviced while those that do not have service but perhaps should I will describe as underserviced.\n\n# Literature Review\n\nThis literature review serves to provide an overview of three important contexts that inform this paper's analytical approach including the policy context, the environmental/limnological/geologic context, and the practice of data science.\n\n## Policy Context\n\nThe goal of this project is to inform multiple autorities and decision makers as they work in collaboration to make and implement public policy. This context, where also multiple sources of authority conflict in their interests, may benefit from an analysis that identifies underserviced or overserviced lakes.\n\nThe management of Wisconsin’s lakes is a shared responsibility that involves state, local, and federal authorities, each contributing unique roles and expertise to ensure sustainable and effective oversight. @garn2003why also point to lake management associations and lake districts as a source of authority, management, and oversight.\n\nAt the state level, the Wisconsin Department of Natural Resources (WDNR) serves as a key authority, providing a framework for lake management. The WDNR’s Lake Modeling Suite offers tools for assessing lake health and predicting management strategy outcomes, while the Surface Water Integrated Monitoring System (SWIMS) serves as a comprehensive database for water quality and ecological data [see @wisconsin_dnr_lake_modeling_suite; and @wisconsin_dnr_swims].  These resources are classic examples of those that data science practitioners would reference in executing a study with this study's analytical goals.\n\n::: {.callout-note} \nThe Wisconsin Department of Natural Resources' Surface Water Integrated Monitoring System (SWIMS) exemplifies the kind of data that data scientists often encounter in applied, policy-relevant contexts. Like many administrative and environmental datasets, SWIMS is rich with information but also characteristically messy. Analysts working with these or similar data must address missing values and inconsistencies in field formats, among other oddities. For example, some columns store numbers as strings due to the presence of unit labels (such as \"43 FEET\"), while others contain values outside the expected or valide ranges. Data of this kind demands both exploration and transformation before meaningful analysis can begin.\n:::\n\nThe state does not operate alone [@Thornton2013Stakeholder]. Local city and county authorities play a part in lake management, particularly in implementing localized policies that address specific community needs including whether to establish a park, beach, or boat access. For example, local governments often plan roads that would service the accommodations and oversee zoning law that would regulate which manner of development may be allowed in a given area. \n\nFederal authorities also contribute to the management of Wisconsin’s lakes through regulatory and funding mechanisms. The federal Environmental Protection Agency (EPA) establishes water quality standards and provide financial support for restoration projects under the Clean Water Act [@Elbakidz2021hetero] and similar initiatives. Additionally, federally funded programs contribute to the preservation of critical habitats and mitigation of invasive species, which are pressing concerns for many of Wisconsin’s lakes.\n\nIn addition to state, local, and federal authorities, tribal governments play a role in the management and preservation of Wisconsin’s lakes [@Waller2018Ecology; and @Spear2020Application]. Many of Wisconsin’s lakes are located on or near tribal lands, and the sovereign rights of indigenous nations give them a unique and essential role in overseeing natural resources. Tribal authorities often have deep, place-based knowledge and cultural ties to these waters, which guide stewardship practices [@Chief2014Engaging].\n\nThe overlapping jurisdictions and collaborative efforts among these levels of government underscore the complexity of lake management. For data scientists working in this domain, understanding the policy context is crucial for framing analytical questions, interpreting data, and ensuring that findings are actionable for stakeholders across all levels of governance. Because this analysis may inform policy decisions across various levels of government (state, local, tribal), the output of this analysis may be useful by many.\n\n## Environmental Limnological Geologic\n\nAs a study of Wisconsin lakes, this work intersects at least three major academic and scientific domains: environmental science, limnology, and geology. In practice a data scientist would heavily rely on literature and also consultations with experts who study, know, and understand these domains. Knowledge from literature and from consultation is commonly called \"domain knowledge\" among data science practitioners [@nelson2023confident]. The ability to contextualize data is further emphasized by [@carvalho2024new], who notes that collaboration with domain experts is vital throughout the data science process to ensure that the results are relevant and actionable. \n\nAccording to @bendor2013modeling the following factors figure into park development: \n\n- Road and highway access: The accessibility of a lake by road or highway.\n- Proximity to public utilities: The availability of public utilities including electricity, septic, sewer, and similar.\n- Size and scope of the park: Larger parks with more complex infrastructure will take longer to plan and build.\n- Land acquisition: Acquiring land can be time-consuming, especially if it involves negotiations, legal proceedings, or eminent domain.\n- Environmental impact assessment: Environmental reviews and permits can add significant time to the process.\n- Funding: Securing funding for land acquisition, design, and construction can be a major hurdle and can delay the project.\n- Public input and approval: Public hearings and comment periods can extend the timeline, especially if there are significant concerns or opposition.\n- Construction timeline: The actual construction phase can vary depending on the complexity of the project, weather conditions, and other factors.\n\nEach of these domains contributes essential perspectives to understanding the complex systems governing lake ecosystems and their management.\n\n::: {.callout-note}\nDomain knowledge refers to the specialized understanding and expertise within a particular field or subject area. In the context of this study, domain knowledge spans environmental science, limnology, and geology—fields that provide the essential insights needed to analyze and interpret data about Wisconsin lakes. It allows data scientists to frame relevant questions, choose appropriate methodologies, and ensure that their findings are both accurate and actionable for decision-makers. [@nelson2023confident].\n:::\n\nThe environmental domain encompasses the broader ecological interactions affecting Wisconsin lakes. This ecology includes factors such as land use, agricultural, water quality, and climate changes. Limnology, the study of inland waters, focuses specifically on the biological, chemical, and physical characteristics of lakes and their surrounding environments. The geologic domain provides insights into the underlying structure and formation of Wisconsin’s lake basins.\n\nThe interplay of environmental, limnological, geologic (and other) factors defines the unique character of each lake. Effective policy and management decisions require an integrated understanding of these domains, supported by data-driven insights. By combining these perspectives, this study aims to model how a data science practitioner's work necessarily involves reference to knowledge not specific to data science in collaboration with others.\n\n\n## The Data Science Process\n\n> The goal of generating new knowledge is at the heart of all scientific disciplines, including data science. It is about finding patterns and relationships in the data that can help us make better decisions or solve complex problems. [@nelson2023confident, p 67]\n\nFor a range of reasons data sciecne is also more \"experimental\" in its approaches [@Martins2021earlyprediction] than more traditional inferential statistical work. For example, according to @Martins2021earlyprediction in regards to model or algorithm selection the best machine learning technique is dependent \"on the dataset and on the formulation of the problem\" (p. 172). According to @Breiman2001culture who described more traditional approaches as \"data modeling\" and the more experimental approahces as \"algorithmic\" modeling a key distinction between these two approaches is that for data modeling the main metric as to what may constitute a \"good\" model is a look at goodness of fit while the key metric for algorithmic modeling is predictive accuracy.\n\n@Breiman2001culture defends so-called algorithmic modeling culture by suggesting that the focus among those in the data modeling culture have brewed an environment which has \"lead to irrelevant theory,\" \"questionable scientific conclusions,\" limited or stymied the use of \"more sutiable agorithmic models,\" and prevented work \"on exciting new problems\" (p. 299-200).\n\nA focus on accuracy is not a trivial distinction. The distiction underscores \nthe practical utility of what @Breiman2001culture refers to as algorithmic modeling in addressing complex problems. This paper’s research question, treated as a thought experiment, highlights the utility of prioritizing accuracy over measures like goodness of fit. Specifically, our goal is to produce two actionable lists of lakes:\n\n1. **Overserviced Lakes** Lakes that currently have public accommodations but perhaps should not. These will be lakes that our model incorrectly predicts as lacking public accommodations.\n2. **Underserviced Lakes** Lakes that lack public accommodations but perhaps should have them. These will be lakes that our model incorrectly predicts as having public accommodations.\n\nThus, given this paper's analytical question the results of interest will be incorrect predictions. The flexibility of data science and it's algorithmic modeling culture permits a tailored approach finely tuned based on the needs of policymakers. \n\nFor instance, if policymakers prefer longer lists of lakes to consider, we can adjust the model through parameter tuning and feature selection to increase error rates, thereby expanding the lists. Conversely, if shorter, more focused lists are desired, we can optimize the model to reduce error rates. \n\nAdditionally, as this paper does, we can refine our approach by analyzing the probability of class membership rather than relying solely on binary predictions. A close look at probabilities permits further customization of list lengths to better meet policy objectives.\n\nIn order to solve the complex problem of deciding which lakes need more services and which lakes may have services but not need them, this paper explores patterns and relationships in Wisconsin Department of Natural Resources (WDNR) data by following the eight step data science process as documented by @nelson2023confident and reproduced here in @fig-data-sicence-process. The paper's new knowledge will be a list of lakes that are potentially overserviced along with another list of lakes that may be underserviced.\n\nThis process, as illustrated in @fig-data-sicence-process starts at the upper left under the heading \"Question or Problem\" and moves clockwise as an interative cycle as it also further consists of the following:\n\n1. **State an analytical question or a business problem to solve.**\n2. **Look + check around.** Explore if the question has been answered or if the problem has been solved. Also look to see what other similar questions or problems have been studied. Identify which methods have, or have not, been successful in the past. Also identify potential data sources.\n3. **Justify the work.** Examine the scope of the question or the problem. Determine how answering the question or solving the problem provides value. Determine what additional revenues or efficiencies an answer to the question or a solution to the problem may bring. If the question has previously been answered or the problem previously solved decide if now is a good time to replicate the work (Is there now new data? Or, are there now new methods available?)\n4. **Wrangle the data.** This involves finding, collecting, extracting, summarizing, validating, formulating, organizing, reshaping, coding, and recoding the data.\n5. **Select and apply a method.** Determine which methods will best answer the analytical question or solve the business problem and then apply those methods.\n6. **Check and recheck.** It is important to look back at this stage. Examine if the analytical question or business problem have been properly stated. Determine if there is new information that may have been missed, discounted, or overlooked in earlier stages. This stage may often involve seeking external input on in-progress work.\n7. **Interpret the results.** Here the process involves answering the original analytical question or proffering a solution to the specified business problem.\n8. **Dissemination.** The last stage involves either, or both disseminating the results or putting them into production.\n\n\n## Dissemination, Not Production\n\nThis paper's example does not exemplify putting a model into production. Below is further explanation as to why putting a model into production is not appropriate for this paper, and indeed the same is also true many papers using data science's algorithmic mode of science.\n\nThis paper's environmental/limnological/geologic contexts are a primary reason why the work here does not involve putting a model into production. A model in production is useful when a business systems, often driven by software systems, require a prediction that can serve as a tool in making a decision. Or in contexts when information is frequently or rapidly changing and the use case calls for using a prediction as a recommendation or an automated decision.\n\n::: {.callout-note}\nA model in production is most useful in environments where predictions must be made repeatedly in response to rapidly changing data. These contexts often involve automated systems or decision-making pipelines—such as in weather forecasting, energy grid optimization, or real-time environmental monitoring—where data streams update frequently, and predictions must adapt continuously. For example, a model deployed in production for natural resources management might monitor streamflow or rainfall to inform automated alerts for flood risks or fire danger. In such cases, real-time data ingestion and dynamic model retraining are essential to the utility of the prediction.\n:::\n\nA description of Wisconsin's lakes from over fourty years ago remains as true today as it was when first published. @lillie1983limnological describe the typical Wisconsin lake as \"natural in origin, equally likely to be of seepage or drainage and stratified or mixed in basic lake type and probably located in the northern half ot the state\" (p. N).\n\nIn the case of Wisconsin's lakes it is not often that a new lake will appear on the map. According to data utilized in this paper's analysis from the WDNR there are {{< var number_of_lakes >}} lakes located throughout the state of Wisconsin. The number of lakes has for decades frequently been reported at a rounded 15,000 (@lillie1983limnological). The number of lakes in Wisconsin, or any geographic region, does not change often. Thus, in this case it is sufficient to train and test a model once on the existing data (which we do not expect to change often).\n\n# Data + Method\n\nAs discussed above the method for this project involves a one-time analysis. Specifically the output will be a list of lakes that do have public accommodations but are more similar to lakes that do not have public accommodations (underserviced). A companion output will be a list of lakes that do not have public accommodations but that are more similar to lakes that do (overserviced).\n\nHere I first discuss the data and then also the method that will produce a list of lakes that should be further considered for the addition of one or more public accommodation and a list of lakes that might benefit from public accommodation closure or retirement.\n\n## WDNR Data {#sec-wdnr-data}\n\nThe WDNR publishes the data for this analysis [@wisconsin_dnr_lake_pages]. The WDNR supports this data, uses it for a full range of purposes [@wisconsin_dnr_swims], and it receives contributions from \"citizen science\" initiatives [@kretschmann2011citizen].\n\nA characteristic lake in this data is {{< var char_lake_name >}} which is about {{< var char_lake_size >}} in acres, up to {{< var char_lake_max_depth >}} deep, (with a mean depth of {{< var char_lake_mean_depth >}}), and located at {{< var char_lake_latitude >}} latitude by {{< var char_lake_longitude >}} longitude in {{< var char_lake_county >}} counties.\n\nBelow @fig-wisconsin-lakes-mapped shows the geographic distribution of lakes across Wisconsin. The lakes with a public beach, park, or boat landing show as dark `x` marks while the lakes with no public beach, park, or boat landing show as small `o` marks. Consistent with estimations from @lillie1983limnological we see in @fig-wisconsin-lakes-mapped that 50% of Wiscosin's lakes are above north 45.5 degress latitude. \n\n::: {.callout-note}\nTo create a geographic visualization of Wisconsin’s lakes, this analysis overlays lake location data onto a geospatial outline of the state. The process relies on several essential tools and geospatial data formats, including GeoJSON, GeoPandas, and the use of a coordinate reference system (CRS) to ensure spatial consistency.\n\nThe geographic outline of Wisconsin is sourced from a publicly available GeoJSON file hosted on GitHub. GeoJSON is a widely used format for encoding geographic data structures using JSON (JavaScript Object Notation). It stores spatial features such as points, lines, and polygons along with associated attribute data. In this case, the polygon defining the boundary of the state of Wisconsin is used as a background layer in the final map.\n\nI provide more thorough discussion of the processes and techniques for building this map visual in the appendicies.\n:::\n\n## Preparation For Analysis\n\nThe following procedure prepares this data for analysis. Starting with a fresh copy of the data from WDNR the code first reports a list of the existing column names and then creates a new set of column names that are more efficient to reference in code. When reviewing the first five columns it appears that max depth and mean depth are stored as strings and contain extraneous text (`\"FEET\"`) data. The code proceeds to remove that extraneous text and then converts the data type to float.\n\nFor a more efficient analysis the code then converts the `haslanding`, `hasbeach`, and `haspark` variables to binary where a `1` replaces the WDNR provided `Yes` value and a `0` replaces the WDNR provided `No` value. The code also creates a `hasservice` variable that is a composite which reports `1` if any of `haslanding`, `hasbeach`, or `haspark` are true and `0` otherwise. The `hasservice` variable will be the primary outcome variable in this analysis.\n\nThe code also inspects the `lat` and `long` data to discover that some values are out of the expected possible range. Wisconsin's southwest corner is at approxaibately 42.5°N by 92.89°W while its northeast corner is at approxaimtely 47.1°N by 86.25°W. These lake data from WDNR include values in the `lat` and `long` columns beyond those ranges. As a preliminary step, where these WDNR data report positive longitude the anlaysis assumes that the intended value was negative to correspond with Wisconsin's location in the western hemisphere. Upon converting the postive longitudinal data values to negative, the remaining non-zero values fall within the expected geographic range. \n\nA total of {{< var zero_coords_count >}} records reported 0°, 0° coordinates. The code also replaces these out of range 0°, 0° data with longitude and latitude values associated with each lake's county coordinates from gigasheet.com. A small number of records contained no county data (or data from multiple counties) and thus the code drops these remaining records from the analysis.\n\nThe newly renamed `fish` column contains a comma separated list of fish species found in each lake. This column also contains a gramatically correct \"and.\" To convert these fish data to an array of dummy columns the code first replace the \"and\" with a comma via `pd.str.replace(' and',',', ')` and then uses `pd.str.get_dummies(sep=', ')`.\n\nThis code also manages missing data as described above in @sec-wdnr-data which describes the data as it was in its oroginal form from WDNR. Three extrememly large lakes in the .018th percential including Lake Winebago are removed. A final inspection of summary statistics is provided in @tbl-prepared-summary.\n\n## Exploratory Data Analysis\n\nExploratory Data Analysis (EDA) serves as a critical bridge between raw data and later more formal analysis and data modeling. This EDA is the stage at which a scientist engages directly with data to uncover initial patterns, surface anomalies, identify missing values, and begin assessing the structure and relationships among variables. @gutman2021becoming describe exploratory data analysis as \"an ongoing process\" (p. 52). Thus EDA is not merely a preliminary step, but rather a dynamic and iterative component of the broader data science workflow. This ongoing process allows the analyst to refine questions, revisit assumptions, and incrementally develop insight into the nature and quality of the data.\n\nAs @nelson2023confident notes, \"without at least some preparation, an exploratory analysis might reveal less than fully useful insights. Simultaneously, without at least some exploration it is not fully possible to know what preparation will be necessary before a full analysis\" (p. 85). This is a *which comes first* problem; a proverbial *chicken or egg* question. For example, in this paper a handful of data manipulations have already been described and executed above, all of which required at least some exploration. Proper execution of EDA early and often through the course of a project guides both data preparation and the analysis along the way. Below is a more formal and analytical exploration of the data aimed at understanding which features may be useful in a predictive algorithm.\n\n**TODO: Add remaining columns to this table. See page 20ish.**\n\n@tbl-prepared-summary presents summary statistics for these lakes data from the WDNR, segmented by whether or not the lake has public services (boat landing, beach, or park). The table reports the mean and standard deviation for each variable within both groups in the left and middle columns, along with pooled overall statistics on the two far right columns. This table provides a summary that can assist in evaluating which variables may serve as useful predictors in classifying or predicting whether a lake has public services.\n\nFor example, lakes with public services are, on average, much larger than lakes without services (126.1 vs. 15.6 acres), and the pooled mean is 51.4 acres. The standard deviation is also substantially higher among lakes with services, reflecting greater variability in size. This  difference suggests that lake size may be a strong candidate as a predictive variable, with larger lakes being much more likely to have public accommodations. Similarly maximum depth (maxdepth), mean depth (meandepth), along with the presence of some fish speacies may also offer predictive value.\n\n## Visual Exploratory Data Analysis\n\nTo explore how each variable may predict the presence of a public service on any of Wisconsin's lakes I produce series of categorical violine plots in @fig-violin-plot. This figure further illustrats how the presence of public services such as boat landings, beaches, or parks may relate to five continuous features of each lake: size, maximum depth, mean depth, latitude, and longitude. Each plot shows the distribution of these features' natural log values for lakes with and without public services.\n\nSpecifically lake size, maximum depth, and mean depth, the top three plots in @fig-violin-plot show differences between lakes that do and do not have public services. In each of these cases, lakes with services (in orange) tend to be shifted to the right along the x-axis, meaning they are generally larger and deeper than lakes without services (in blue). Not only are the central tendencies higher for lakes with services, but the spread of values is also wider, particularly for size, suggesting a greater diversity in lake sizes among serviced lakes. These differences imply that lake size and depth may be strong predictors for whether a lake has public services.\n\nOn latitude and longitude, the bottom two plots of @fig-violin-plot, show less pronounced differences. There is modest separation in the distributions for lakes with and without services, particularly in latitude, where lakes with public services appear to be slightly more concentrated in certain geographic bands. This may reflect regional planning priorities or population density factors but likely provides less predictive value than physical lake characteristics.\n\n::: {.callout-note}\nThe continuous variables displayed in Figure 4 have long-tailed, highly-skewed distributions. To make the visualizations more interpretable and to reduce the influence of extreme values, I applied the natural logarithm transformation. This common transformation compresses the scale of larger values. The result is a more readable distribution.\n:::\n\nThese visual explorations serve as an important visual check on the potential predictive power of each feature when modeling the presence of public service. The greater the separation between the two distributions, the more likely that variable will be useful in a classification task. Based on the WDNR data, features like size, max depth, and mean depth appear to have strong potential as predictors, while latitude and longitude may contribute some additional nuance when combined with other variables.\n\nOne additional customary plot used in exploratory data anlysis is the pair plot. Below @fig-logpairplot_1 again reveals the same patterns noted above in @tbl-prepared-summary and @fig-violin-plot.\n\n## Binary Variable Exploration\n\nThese WDNR data also consist of multiple columns of a binary nature, reporting the presence of specific fish species including catfish, largemouth bass, musky, northern pike, panfish, smallmouth bass, sturgeon, trout, and walleye. Instead of exploring the potential predictive value of these binary with violin plots we must turn to other options such as chi square analysis which is well suited to test the whether the presence of a publice service may be a function of the presence of any given species.\n\n## Machine Learning Predictions As Recommendations\n\n@james2023introduction explain that \"broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs\" (p. 1). Data science methods genreally aim to leverage a series of features which can reliably predict an outcome. \n\nIn this paper's analysis the outcome is whether a lake has as public service while the predictors are each lake's size in acers, maximum depth, mean depth, latitude, longitude, type ({{< var lake_type_vals >}}), clarity ({{< var lake_clarity_vals >}}), and the presence of specific fish species ({{< var fish_species_list >}}).\n\nThrough the use of a predictive algorithm that measures similarity this paper's analysis will first create a model that looks to predict which of Wisconsin's lakes have a public service and which of Wisconsin's lakes do not have a public service. Inevitably there will be some errors.\n\nOne set of errors will be lakes that do not have a service but that the model predicted would (becauase they are similar to others that do) which we will call overserviced. Likewise another set of errors will be lakes that do have a service but that model predicted would not (becuase these other lakes are more similar to those that do not have service) which we will call underserviced.\n\n## k-Nearest Neighbors (KNN)\n\nTo classify whether a lake should or should not have a public service, this paper turns to a classic algorithm known as k-Nearest Neighbors (KNN). More traditionally, this algorithm serves to predict class membership based on an observation's similarity to other observations of a known classification.\n\nKNN is a non-parametric, instance-based learning algorithm that does not make assumptions about the underlying distribution of the data. Instead, it classifies instances based on the majority class of their closest neighbors in a feature space. Which is a technical manner of explaining that KNN is a way for a computer algorithm to make predictions or conclusions about an object based on what it \"sees\" (or measures using euclidian distances) as other similar objects. \n\nA simplified version of the KNN process, as it operates for this paper, is to first observe an unknown lake and its characteristic features. Suppose the unknown lake is less than one acre in surface area and also less than 6 feet deep at its max. Then further suppose that 99% of all lakes less than an acre in size and also around 6 feet deep (+/- one foot) have no public service. Thus it would appear that the unknown lake is similar to lakes with no service and accordingly the algorithm would predict the uknown lake as one that also would have no public services.\n\nIn data science we use the term *non-parametric* to describe predictive algorithms, such as KNN, which have no fixed formula. Being absent a fixed formula distinguishes the non-parametic approach from parametric approaches, such as logistic regression, for example. @james2023introduction describe KNN as one of \"the simplest and best-known non-parametric methods\" (p. 111).\n\nModels that are *instance-based* make decisions by comparing one instance (or given this paper's data any instance of a given lake) to others instances (other lakes). During the model fit procedure the algorithm memorizes the training data by storing as a reference all training instances. A *feature space* is a term for the way we describe the multi-dimensional, or multi-variate, nature of the predictive features and their values.\n\n::: {.callout-note}\nThis distance metric must not be conflated for geographic distance. For example, two lakes located on opposite sides of the state may still be considered \"close\" in feature space if they share similar size, depth, water clarity, and fish populations. This abstract notion of distance allows KNN to make predictions based on overall similarity in characteristics, rather than geographic location alone.\n:::\n\nIn this paper's data, the feature space includes dimensions such as lake size, maximum depth, mean depth, latitude, longitude, water clarity, and the presence of specific fish species. The KNN algorithm uses this feature space to calculate distances between lakes and identify their nearest neighbors, which helps classify them based on their similarity.\n\n# Analysis\n\nThis analysis uses the open-source Python package Scikit-learn (https://scikit-learn.org), a widely adopted and well-documented framework for implementing machine learning models, including k-nearest neighbors (KNN). Scikit-learn supports reproducible and transparent research, making it ideal for applications in data science and policy evaluation.\n\n**TODO: Add Sklearn documenation as a source.**\n\nThe analytical procedures in this paper align with those outlined in Chapter 11 of @nelson2023confident and Chapter 3 of @raschka2019python. Most machine learning workflows begin by splitting the available data into training and testing sets. The training set allows the model to learn patterns and relationships in the data, while the testing set remains untouched until final evaluation. This split helps ensure that performance estimates reflect how the model will generalize to new data not seen during training.\n\nThe training and testing sets also support model parameter selection. In this case the parameter to optimize is the optimal number of *k* neighbors in the KNN algorithm. As is also customary, to avoid information leakage, this analysis performs data preprocessing after the data has been split. Binary features (such as fish species presence) require no further transformation. However, nominal features such as clarity and type are converted into dummy variables using one-hot encoding, and all continuous predictors are rescaled using a standardization procedure to ensure they contribute equally to distance calculations.\n\nFollowing transformation, the next step involves conducting a parameter search to identify the most effective value for *k*, the number of nearest neighbors used in the KNN algorithm. In most cases, the customary approach is to select the smallest value of *k* that also minimizes prediction error. A smaller *k* yields a simpler, more interpretable model. However, this paper intentionally selects a less-well performing k value to relize a more complex model. The reasoning behind this choice is practical: a more complex model yields a finer-grained and more complete distribution of predicted probabilities. These probabilities in turn, support a more nuanced analysis of false predictions. As the anslysis seeks to identify lakes that do not have public services but appear similar to those that do, and vice versa. This added granularity enhances the utility of the model in producing actionable policy recommendations.\n\nAs a final step, this analysis uses a two-fold cross-validation with symmetric evaluation, which ensures that every lake is evaluated as an out-of-sample observation exactly once. In the first fold, the model is trained on half of the data and used to predict outcomes for the other half; in the second fold, the roles are reversed. This method yields a complete set of out-of-sample predictions, which allows us to identify false positives (lakes without public services that resemble lakes that do) and false negatives (lakes with public services that resemble those that don’t). These classification errors serve as the empirical foundation for the policy recommendations presented later in this paper.\n\n## Train Test Split Procedure\n\nThe SciKit Learn user guide states plainly that \"Learning the parameters of a prediction function and testing it on the same data is a methodological mistake\" @scikit-learn-cross-validation2023. This mistake would usually lead to a model that has *overfit*. Instead of finding the general functional relationships between predictor features and an outcome, a model that has overfit to the data, has come close to memorizing the training data.\n\nBy first fitting a model on a subset of training data and then using a separate hold out subset as a test, the procedure results in a more objective opportunity to fairly evaluate the predictive abilities of a model. The procedure ensures that a model which performs well on training data also later performs well on new data yet to be generated in future production settings.\n\n## Rescale Continuous Data\n\nAccording to @raschka2019python \"the majority of machine learning... algorithms behave much better if features are on the same scale\" (p. 124). In the case of KNN, an algorithm that relies on calculating distances between data points in the feature space, if the features spread across vastly different scales, those with larger ranges will dominate the distance calculations. \n\nFitting a KNN on features that are differently scaled may potentially lead to biased classifications. For instance, if one feature represents lake size in acres (ranging from tens to thousands) and another represents water clarity on a scale of 1 to 100, the size feature will disproportionately influence the nearest neighbor determination. Thus, For KNN, rescaling ensures that all features contribute equally to the distance metric [@nelson2023confident, p. 317].\n\nTo address this issue, continuous data is typically rescaled to 0 to 1, -1 to 1, or to z-score values which places the mean at 0 and then shifts values so there is a standard deviation of 1. In this paper I proceed with a 0 to 1 scale.\n\n## Fit Base Model\n\nBefore proceeding with parameter tuning or model refinement, it is often useful to fit a base model using an arbitrary but reasonable choice of parameters. In the case of k-nearest neighbors (KNN), selecting a base *k* such as *k* = 19. This base model provides an initial benchmark for model performance. This step, though technically optional, serves several important purposes within the broader analytical process.\n\nFirst, fitting a base model allows the analyst to ensure that the pipeline—from data preprocessing to model training and evaluation functioned as intended. Errors related to data structure, scaling, encoding, or other preprocessing steps often surface during this preliminary fit. The base model thus acts as a diagnostic opportunity to detect problems before introducing additional complexity through cross-validation, parameter searches, or other hyperparameter tuning efforts.\n\nSecond, the base model offers a reference point for evaluating the value added by subsequent modeling decisions. For example, if the accuracy or error rate of a tuned model differs only marginally from that of the base model, the analyst may reconsider the complexity or interpretability trade-offs involved in optimization. Conversely, large improvements over the base model suggest meaningful gains that justify further refinement.\n\nFinally, the base model supports replication and transparency by providing a fixed and reproducible result that others can use to validate or extend the work. By producing and recording model performance with arbitrary but documented parameters, the analysis builds a foundation upon which subsequent results can be compared, especially in applied contexts where interpretability and policy implications matter as much as technical performance.\n\nIn short, while fitting a base model with arbitrary *k* is not strictly necessary for most analyses, it offers extensive practical value in building a rigorous, transparent, and well-structured analysis.\n\nThis base model yeild {{< var base_accuracy >}} accuracy (correct classifications). There were {{< var base_false_pos >}} false positive predictions and {{< var base_false_neg >}} false negative predictions. These base metrics can serve as a helpful reference when evaluating futher results below.\n\n## Evaluate + Search for Optimal K\n\nThis portion of the analysis implements a parameter search to determine how the choice of *k* (the number of neighbors considered in the KNN classification algorithm) affects model performance. The goal is to identify a value of *k* that yields relatively low classification error, thereby improving the model’s predictive accuracy. As discussed above, for this paper I will not choose the lowest error rate in order to support a fully nuanced analysis.\n\nThis portion of the analysis begins by initializing an empty list named `error_rates` to store the error rate associated with each value of *k*. The `for` loop then iterates through odd-numbered values of *k* from 1 to 99. Odd values avoid tie votes in binary classification. For each iteration, the code instantiates a new `KNeighborsClassifier` model using the current value of *k* and fits it to the training data.\n\nOnce trained the code predicts classifications for the testing set (`X_test`), then calculates the error rate as the proportion of incorrect predictions, and appends that result to the `error_rates` list. By the end of the loop, the list holds the model's error rates across a range of *k* values.\n\nA subsequent block of code generates @fig-error_rates, a visual representation of the results using Matplotlib. @fig-error_rates uses a dotted blue line with ‘x’ markers to show how error rates change as *k* increases.\n\nBy examining @fig-error_rates's curve, an analyst can make an informed decision about which *k* values to consider for the final model—balancing error rate, model simplicity, and practical interpretability. Given these results I choose a *k* value of 29 consistent with the procedure outlined above.\n\n## Two-fold cross-validation; Symmetric evaluation\n\nIn executing this procedure I first split the data into two equal halves. In the first fold, I train the model on the first half and predict on the second; in the second fold, the I reverse the process.\n\nAs before, within each fold, continuous variables are standardized using `StandardScaler` and nominal categorical variables are transformed via one-hot encoding using `OneHotEncoder`.\n\nAfter each fit, train, and predict I record both the prediction and also the probability of membership in the `hasservice` class. Finally, I combine predictions from both folds and merged them back into the original data. The result is a full set of out-of-sample predictions and prediction probabilities for each observation.\n\n# Results\n\nThis analysis produced {{< var false_preds_count >}} false predictions. A full list of all the lakes analyzed in this study and the predictive results accompanies this project in a file called `output_data.csv`. \n\n## Underserved Lakes\n\nUnderserved lakes are the lakes that this analysis falsely predicted not to have services, but that actually do have services. Because the results produced {{< var false_positive_count >}} false positives I further refine the definition of underserved lakes to include those that had a probability of service greater than .80. @tbl-overserviced-lakes provides a full list of underserviced lakes.\n\n\n<div style=\"font-size: 12px;\">\n\n</div>\n\n@tbl-underserviced-lakes presents a list of lakes identified by KNN model as false positives, or as lakes that were predicted to have public services but do not currently offer them. Each row includes the lake's name, its surface area in acres (`size`), geographic coordinates (`lat` and `lon`), the county or counties in which the lake is located, and the model's estimated `probability`. \n\nThis `probability` value is a key element in the table: it represents the proportion of the 29 nearest neighbors (as set by the optimal *k* in the model) that do have a public service. For example, a probability of `0.827586` implies that 25 out of 29 neighboring lakes in the feature space had a public service, making the model highly confident that the lake in question should as well.\n\nLakes such as Kilbourn Flowage, Kettle Moraine Lake, and Fence Lake have especially high probabilities—approaching or reaching 1.0—indicating that all of their most similar lakes in the multi-dimensional feature space do have services. These high-probability false positives are particularly important because they suggest a strong pattern of similarity when compared to serviced lakes and thus may warrant prioritization for future investments in public amenities.\n\n## Overserviced Lakes\n\nThe opposite of underserved lakes, overserviced lakes are those that the model suggested would not have services but do (the false negatives). With {{< var false_negative_count >}} false negatives it is also convenient to refine this definition. To reduce the number of lakes for consideration as overserviced I further refine the definition of overserviced lakes to only include those that had a probability of service less than .03. @tbl-underserviced-lakes provides a full list of these overserviced lakes.\n\n\n<div style=\"font-size: 12px;\">\n\n</div>\n\n@tbl-overserviced-lakes identifies lakes classified as false negatives by the KNN model. These lakes do have public services, but the model predicted they would not. In this analysis, such lakes are referred to as overserviced, meaning they are dissimilar—based on the features available—to most other lakes that also have services.\n\nAs was also with @tbl-underserviced-lakes this `probability` column reflects the proportion of the 29 most similar lakes (i.e., nearest neighbors in the multi-dimensional feature space) that had public services. A `probability` of 0.0 implies that none of the 29 neighbors had a service.\n\nFor example, Fountain City Bay and Ceylon Lagoon offer public services but are quite like at least 29 other unserviced lakes. Ultimately, @tbl-overserviced-lakes and @tbl-underserviced-lakes provide a data-driven starting point for reevaluating the current distribution of public services across Wisconsin’s lakes by identifying outliers that may represent opportunities for improved alignment with service needs. Whiel @fig-knn-false-predictions shows the location of these underserviced and overserviced lakes.\n\nThe map in @fig-knn-false-predictions presents a visualization of false predictions generated by the KNN classification model trained to identify which Wisconsin lakes have public services—such as boat landings, beaches, or parks—based. Specifically, it highlights two important categories of interest for policy consideration: underserved lakes and overserviced lakes.\n\n# Further Discussion\n\nLakes marked with blue \"x\" symbols represent false positives, or underserved lakes. These underserved lakes are those that the model predicted to have public services, because they are similar to other lakes that do, but that in reality do not have public services. These lakes may warrant further investigation as candidates for the addition of public accommodations.\n\nConversely, lakes marked with black dots represent false negatives, or overserviced lakes. These overserviced lakes are those where the model predicted no public services should be present but that are currently are serviced with boat landings, beaches, or parks. These may be examples of lakes whose public services might be considered for decommission or retirement.\n\nBoth underserved and overserved lakes are widely distributed throughout the state. This spatial analysis offers a critical bridge between predictive modeling and those audiences who may consume this information as they look to make and implement lake management policy. By identifying specific lakes for which the model's prediction diverges from current reality, this map serves as a guide for natural resource managers, planners, and local officials for further review.\n\n## Limitations + Weaknesses\n\n{{< lipsum 3 >}}\n\n## Suggestions For Further Work\n\n{{< lipsum 2 >}}\n\n\n\n\n# Summary + Conclusions\n\n{{< lipsum 4 >}}\n\n# Appendicies\n\n**Mapping Wisconsin’s Shape and Lake Locations**\n\nTo generate the maps featured in this analysis, the shape of the state of Wisconsin was retrieved from a public **GeoJSON** file hosted on GitHub. GeoJSON is a geospatial data format based on JSON (JavaScript Object Notation) that stores geographic features such as points, lines, and polygons. In this case, the boundary polygon of Wisconsin serves as the base layer for the map.\n\nThe file is read into a **GeoDataFrame** using the `GeoPandas` library (`gpd`). GeoPandas is an extension of Pandas that enables spatial operations and plotting by introducing a `geometry` column capable of storing shapely geometries. A simple function was written to check whether the Wisconsin GeoJSON file already exists locally and, if not, to download and store it from a known remote source.\n\nLake location data from the Wisconsin Department of Natural Resources (WDNR) was merged with additional columns indicating the presence of public services. Using `gpd.points_from_xy()`, each lake’s longitude and latitude values were converted to a point geometry, and the data were also cast into a GeoDataFrame.\n\nTo ensure proper alignment on the map, both the Wisconsin boundary and the lake point geometries were assigned the same **Coordinate Reference System (CRS)**: EPSG:4326, which corresponds to the standard WGS84 system using degrees of latitude and longitude.\n\n- EPSG:4326 refers to a specific coordinate reference system (CRS) that is widely used for geographic data. It is part of a registry maintained by the European Petroleum Survey Group (EPSG), which assigns numeric codes to standard spatial reference systems. EPSG:4326 specifies that coordinates are expressed in degrees of latitude and longitude, based on the WGS84 datum.\n\n- WGS84, or the World Geodetic System 1984, is a global reference system used by GPS and many mapping applications. It defines the shape of the Earth as an ellipsoid and provides a consistent framework for locating points on the Earth's surface. When spatial data is aligned to WGS84, it can be accurately mapped and compared across different datasets and tools, making it a common default in global mapping applications and open geospatial data formats.\n\nPlotting was accomplished using **Matplotlib** and **Seaborn**, with lake points styled according to the presence or absence of public services (e.g., boat landings, beaches, or parks). This overlay provides a spatial perspective that supports both exploratory analysis and the interpretation of model predictions.\n\n# Scratch Space\n\n# Outtakes\n\nThis paper demonstrates a data science process that doesn't require a continually updated model. Unlike situations where data changes frequently (e.g., online shopping recommendations), the number of lakes in Wisconsin is relatively static. Our goal is to provide a one-time analysis identifying potential discrepancies between existing public services and lake needs. This information will be valuable for policy discussions across various government levels, where a single, well-documented analysis is more suitable than a dynamic model.\n\n\n\n\nMoreover, according to @Srinivas2022feature domain knowledge plays a critical role in feature engineering, a key step in the data science pipeline. Srinivas et al. argue that while automation can assist in expanding the feature space, it cannot replace the human ability to apply domain-specific knowledge to identify meaningful features. This sentiment is echoed in the work of @Waller2013bigdata who assert that the intersection of data science with specific domains necessitates a deep understanding of both the data and contexts in which it is applied. Without this understanding, data scientists may struggle to derive insights that are both relevant and useful.\n\n\n\"Overfitting is a common problem in machine learning, where a model performs well on training data but does not genrealize well to unseen datas (test data). If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters, leading to a model that is too complex given the underlying data.\" [@raschka2019python, p 75]\n\n# References\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":true,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"output-file":"ParkSiteSelection.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","yaml":"_variables.yml","title":"Applied Reproducible Data Science Processes","subtitle":"A demonstration using data from Wisconsin lakes","author":[{"name":"Adam Ross Nelson JD PhD","email":"arnelson3@wisc.edu","affiliation":"University of Wisconsin - Madison"}],"date":"January 1, 2025","bibliography":["references.bib"],"jupyter":"python3","toc-title":"Table of Contents","embeded-resources":true},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":6,"fig-height":4,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"number-sections":true,"output-file":"ParkSiteSelection.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"yaml":"_variables.yml","title":"Applied Reproducible Data Science Processes","subtitle":"A demonstration using data from Wisconsin lakes","author":[{"name":"Adam Ross Nelson JD PhD","email":"arnelson3@wisc.edu","affiliation":"University of Wisconsin - Madison"}],"date":"January 1, 2025","bibliography":["references.bib"],"jupyter":"python3","documentclass":"article","margin-left":"1in","margin-right":"1in","margin-top":"1in","margin-bottom":"1in","header-includes":["\\usepackage{draftwatermark}\n\\DraftwatermarkOptions{%\n  text={INPROGRESS},\n  color={[rgb]{1,0.6,0.6}}, % light red\n  angle=45,\n  scale=.5\n}\n"]},"extensions":{"book":{"selfContainedOutput":true}}},"docx":{"identifier":{"display-name":"MS Word","target-format":"docx","base-format":"docx"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"docx","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"page-width":6.5},"pandoc":{"default-image-extension":"png","to":"docx","reference-doc":"custom-reference.docx","output-file":"ParkSiteSelection.docx"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"yaml":"_variables.yml","title":"Applied Reproducible Data Science Processes","subtitle":"A demonstration using data from Wisconsin lakes","author":[{"name":"Adam Ross Nelson JD PhD","email":"arnelson3@wisc.edu","affiliation":"University of Wisconsin - Madison"}],"date":"January 1, 2025","bibliography":["references.bib"],"jupyter":"python3"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":[]}